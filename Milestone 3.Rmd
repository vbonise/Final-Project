---
title: "Milestone 3"
author: "Tori Bonisese"
date: "April 28, 2018"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Git Repo
[https://github.com/vbonise/Final-Project.git](https://github.com/vbonise/Final-Project.git)

# Data Sources
Data is gathered from the Center for Medicare and Medicaid Services' (CMS)  Hospital Compare website  [https://www.medicare.gov/hospitalcompare/HAC-reduction-program.html](https://www.medicare.gov/hospitalcompare/HAC-reduction-program.html), [https://data.medicare.gov/data/archives/hospital-compare](https://data.medicare.gov/data/archives/hospital-compare), and CMS's Inpatient Prospective System for years 2016-2017 [https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/AcuteInpatientPPS/FY2016-IPPS-Final-Rule-Home-Page.html](https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/AcuteInpatientPPS/FY2016-IPPS-Final-Rule-Home-Page.html),  [https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/AcuteInpatientPPS/FY2017-IPPS-Final-Rule-Home-Page.html](https://www.cms.gov/Medicare/Medicare-Fee-for-Service-Payment/AcuteInpatientPPS/FY2017-IPPS-Final-Rule-Home-Page.html).


#Explanation of process thus far:
**Objective**
This will be a sub-analysis of my thesis - I am interested to know if prior penalization is predicitive of future performance and, rather than do this analysis in STATA, I will use this project to answer that question. In addition, this question is important to me because it highlights the concern that lower performing hospitals may never escape penalization regardless of improvement. This needs to be addressed as incentives may need to be adjusted in future policy.

**Workflow**
I acquired my data from Hospital Compare and CMS and did a majority of the data cleaning for my thesis in SAS and Stata. For this particular project I took a dataset from my thesis that combined data from Hospital Compare and the Inpatient Prospective System from CMS. First, I ran descriptive statistics to get a "picure" of my data. I used chi-square analysis and t-tests to assess differences between safety-net and non-safety-net hospitals as safety-net hospitals are my focal variable. These tables are documented below and the descriptive statistics are as expected. 
In order to answer my question of how penalization of a safety-net hospital in one year affects odds of penalization in the following year, I needed to take my dataset from its long format to wide. I needed to do this because I am interested in how many times a hospital was fined from 2015-2017 and, as far as I know, R "thinks" across and then down, so to know the number of penalizations by provider ID I need to have the penalization years horizonatally adjacent to eachother rather than vertically. I used the reshape2 package to change the data from long to wide, and then merged the resuts back onto the controls so that I could run the logisitc regression. Next, I created two subsets of data - one for 2015 and one for 2016. These subsets were created conditional on a hospital being penalized in 2015 and 2016, respectively. Subsetting the data allows me to isolate penalized hospitals and run a logistic regression to assess the odds of penalization in the following year. The subsets and logistic regressions are documented below. Lastly, I converted my results to odds ratios as they are much easier to interpret and explain to an audience. I do not have any ancillary questions yet, but I'm sure I will as time progresses. I have not deviated from my original plan.

#Import Data & Call Packages
```{r}
devtools::install_github("lbusett/insert_table")
library(haven)
library(car)
library(reshape2)
library(aod)
library(ggplot2)
library(dplyr)
library(stargazer)
FinalScores_All3Years <- read_dta("FinalScores_All3Years.dta")
View(FinalScores_All3Years)
```

##Descriptive Stats
###I am doing this to understand the data distribution and stats 
```{r}
library(gmodels)
invisible(table(FinalScores_All3Years$dshpct))
invisible(table(FinalScores_All3Years$teach, FinalScores_All3Years$dshpct))
invisible(table(FinalScores_All3Years$beds_small, FinalScores_All3Years$dshpct))
invisible(table(FinalScores_All3Years$beds_medium, FinalScores_All3Years$dshpct))
invisible(table(FinalScores_All3Years$beds_large, FinalScores_All3Years$dshpct))
tapply(FinalScores_All3Years$cmi, FinalScores_All3Years$dshpct, mean)
invisible(table(FinalScores_All3Years$urban, FinalScores_All3Years$dshpct))

##Now do cross tabs with pvals to understand distribution and siginificant differences across hospital types
CrossTable(FinalScores_All3Years$dshpct, FinalScores_All3Years$teach, chisq=TRUE, format="SAS")
CrossTable(FinalScores_All3Years$dshpct, FinalScores_All3Years$beds_small, chisq = TRUE)
CrossTable(FinalScores_All3Years$dshpct, FinalScores_All3Years$beds_medium, chisq = TRUE)
CrossTable(FinalScores_All3Years$dshpct, FinalScores_All3Years$beds_large, chisq = TRUE)
CrossTable(FinalScores_All3Years$dshpct, FinalScores_All3Years$urban, chisq = TRUE)
CrossTable(FinalScores_All3Years$dshpct, FinalScores_All3Years$vbp_fine, chisq = TRUE)
CrossTable(FinalScores_All3Years$dshpct, FinalScores_All3Years$readmiss_fine, chisq = TRUE)
CrossTable(FinalScores_All3Years$dshpct, FinalScores_All3Years$fine, chisq = TRUE)
t.test(cmi~dshpct, data = FinalScores_All3Years)

##Check number fined by year to ensure no misscoding
nrow(filter(FinalScores_All3Years, fine == 1 & fiscal_year == 2015))
nrow(filter(FinalScores_All3Years, fine == 1 & fiscal_year == 2016))
nrow(filter(FinalScores_All3Years, fine == 1 & fiscal_year == 2017))
```

##Transform from long to wide
Here I am transforming from long to wide so that I can sum the number of times a hospital has been fined across 2015-2017.

```{r}

test <- head(FinalScores_All3Years)

###FinalScoresWide <- melt(FinalScores_All3Years, id= "provider_id", "fiscal_year")
widetest <- dcast(test, provider_id ~ fiscal_year, value.var = "fine")
FinalScores_Wide <- dcast(FinalScores_All3Years, provider_id ~ fiscal_year, value.var = "fine")

##Relabel to append to original dataset
colnames(widetest)[2] <- "fine15"
colnames(widetest)[3] <- "fine16"
colnames(widetest)[4] <- "fine17"

##do for actual dataset, not just test
colnames(FinalScores_Wide) [2] <- "fine15"
colnames(FinalScores_Wide) [3] <- "fine16"
colnames(FinalScores_Wide) [4] <- "fine17"

##Merge to original dataset
total <- merge(FinalScores_All3Years,FinalScores_Wide,by="provider_id")

##Check to see if worked
library(Hmisc)
describe(FinalScores_Wide)
###there are distinct 0 & 1 for all values of fine
table(FinalScores_Wide$fine15)
table(FinalScores_Wide$fine16)
table(FinalScores_Wide$fine17)

##Create indicators using old code above now that data is wide
total <- mutate(total, fine_all = (if_else (fine15 == 1 & fine16 ==1 & fine17 == 1, 1, 0)))
test_all <- select(total, fine, fine_all)
describe(test_all)
table(total$fine_all)
```

##Do logistic regression
```{r logit}
##Need to subset the data for those fined in 2015
##I am doing this because I am interested in the odds of a safety-net hospital being fined in 2016 conditional on their being fined in 2015
FinalScores_Subset15 <- select(filter(total, fine15==1), c(provider_id, fiscal_year, fine, fine15, fine16, fine17, dsh, teach, urban, beds_small, beds_medium, beds_large, readmiss_fine, vbp_fine))

#Need to subset the data for those fined in 2016
##I am doing this because I am interested in the odds of a safety-net hospital being fined in 2017 conditional on their being fined in 2016
FinalScores_Subset16 <- select(filter(total, fine16==1), c(provider_id, fiscal_year, fine, fine15, fine16, fine17, dsh, teach, urban, beds_small, beds_medium, beds_large, readmiss_fine, vbp_fine))

#Now, run logits

##Odds of being fined in 2016 after being fined in 2015
logit16 <- glm(fine16 ~ dsh + teach + urban + beds_medium + beds_large +readmiss_fine + vbp_fine, data = FinalScores_Subset15, family=binomial(link="logit"))

summary(logit16)

##Odds of being fined in 2017 after being fined in 2016
logit17 <- glm(fine17 ~ dsh + teach + urban + beds_medium + beds_large +readmiss_fine + vbp_fine, data = FinalScores_Subset16, family=binomial(link="logit"))

summary(logit17)

##Odds of being fined in 2017 after being fined in 2015, because 2016 results were counter to my expectation, I am curious about this
logit17_15 <- glm(fine17 ~ dsh + teach + urban + beds_medium + beds_large +readmiss_fine + vbp_fine, data = FinalScores_Subset15, family=binomial(link="logit"))

summary(logit17_15)
###safety-net hospitals are significantly more likely to be fined, as expected. Checked 2016 data and there are no errors. Thoughts for discussion section are note the increasing difficulty of meeting the score every year as it decreases and SNs limited resources, should see similar results for 2018 and beyond (unfortunately.)

##This logit will assess the odds of safety-net hospitals being fined in all 3 years
logit_all <- glm(fine_all ~ dsh + teach + urban + beds_medium + beds_large +readmiss_fine + vbp_fine, data = total, family=binomial(link="logit"))

summary(logit_all)

#get ORs
exp(coef(logit16))
exp(coef(logit17))
exp(coef(logit17_15))
exp(coef(logit_all))

```

#Tables are be placed below, P values: *p < 0.05, **p < 0.01

##Table 1. Descriptive Statistics 2015-2017
This table shows how the control variables are distributed across the variable of interest (safety-net hospitals) and non-safety-net hospitals. The table is read such that of all safety-net hospitals, 44.48% are teaching hospitals, which is significantly different than the percent of non-safety-net hospitals that are also teaching hospitals (28.56%).
```{r}
Descriptive_Statistics_2015_2017 <- tibble::tribble(
  ~Variable,    ~Safety.Net., ~Non.Safety.Net,   ~P.Value,
                                                 "Hospital Type", "2374 (25.31%)", "7007 (74.69%)",         NA,
                                             "Teaching Hospital", "1056 (44.48%)", "2001 (28.56%)",  "0.00***",
                                             "Small (<100 Beds)",  "613 (25.82%)", "2749 (39.23%)",  "0.00***",
                                       "Mid-Size (100-399 Beds)", "1302 (54.84%)", "3614 (51.58%)", "0.01*** ",
                                             "Large (>399 Beds)",  "459 (19.33%)",   "644 (9.19%)",  "0.00***",
                                                "Urban Location", "1784 (75.15%)", "5268 (75.18%)",     "0.97",
                                                "Fined by HACRP",  "693 (29.19%)", "1468 (20.95%)",  "0.00***",
                                                  "Fined by VBP", "1239 (52.19%)", "2755 (39.32%)",  "0.00***",
                                                 "Fined by HRRP", "2074 (87.36%)", "5530 (78.92%)",  "0.00***",
                                                      "Mean CMI",          "1.51",          "1.56",  "0.00***"
  )

require(knitr)
kable(Descriptive_Statistics_2015_2017, digits = 3, row.names = FALSE, align = "c",
              caption = NULL)


```

##Table 2. Odds of Penalization in 2016 After Penalization in 2015
This table shows odds ratios from my first logistic regression. Results indicate that teaching hospitals were 1.54 times more likely than non-teaching hospitals to be fined in 2016 after being fined in 2016. Hospitals that were fined by VBP in 2015 were 1.29 times more likely to be fined by the HACRP in 2016. This is unsurprising as VBP uses many of the same measures as the HACRP. Contrary to my hypothesis, there is no significant result for safety-net hospitals.
```{r}
Fine_2016 <- tibble::tribble(
  ~Variable,      ~OR,     ~SE,
                                                   "Non-Safety-Net ",    "Ref",      "-",
                                                        "Safety-Net",   "1.25", "0.127",
                                             "Non-Teaching Hospital",    "Ref",      "-",
                                                 "Teaching Hospital", "1.54**", "0.116",
                                                 "Small (<100 Beds)",    "Ref",      "-",
                                           "Mid-Size (100-399 Beds)",   "1.20", "0.139",
                                                 "Large (>399 Beds)",  "1.65*", "0.203",
                                                             "Rural",    "Ref",      "-",
                                                             "Urban",  "0.992", "0.142",
                                                              "HRRP", "0.71**", "0.125",
                                                               "VBP", "1.29**", "0.098"
  )

require(knitr)
kable(Fine_2016, digits = 3, row.names = FALSE, align = "c",
              caption = NULL)

```


##Table 3. Odds of Penalization in 2017 After Penalization in 2016
This table shows that safety-net hospitals fined in 2016 were 1.9 times more likely to be fined in 2017 than non-safety-net hospitals, teaching hospitals fined in 2016 were 1.17 times more likely to be fined in 2017 than non-teaching hospitals, and hospitals fined by VBP in 2016 were 1.09 times more likely to be fined in 2017 than hospitals not fined by VBP.
```{r}
Fine_2017 <- tibble::tribble(
  ~Variable,      ~OR,     ~SE,
                                                   "Non-Safety-Net ",    "Ref",      NA,
                                                        "Safety-Net", "1.90**", "0.131",
                                             "Non-Teaching Hospital",    "Ref",      "-",
                                                 "Teaching Hospital",   "1.17", "0.116",
                                                 "Small (<100 Beds)",    "Ref",      "-",
                                           "Mid-Size (100-399 Beds)", "2.45**", "0.139",
                                                 "Large (>399 Beds)", "2.51**", "0.201",
                                                             "Rural",    "Ref",      "-",
                                                             "Urban", "1.59**", "0.146",
                                                              "HRRP", "0.96**", "0.121",
                                                               "VBP", "1.09**", "0.100"
  )

require(knitr)
kable(Fine_2017, digits = 3, row.names = FALSE, align = "c",
              caption = NULL)

```

##Table 4 Odds of Penalization in All 3 Years (2015-2017)
This table shows that, compared to non-safety-net hospitals, safety-net hospitals are 2.3 times more likely to be fined in all three years of the dataset. Similar results can be seen for teaching hospitals and VBP. The HRRP result is interesting, however it is not a focus of this particular analysis. It is worth noting there is no overlap in metrics between the HRRP and HACRP. Hospital location and size results may be of interest for subsequent analyses. 
```{r}
Fine_All <- tibble::tribble(
  ~Variable,      ~OR,     ~SE,
                                                   "Non-Safety-Net ",    "Ref",      "-",
                                                        "Safety-Net", "2.31**", "0.107",
                                             "Non-Teaching Hospital",    "Ref",      "-",
                                                 "Teaching Hospital", "1.99**", "0.098",
                                                 "Small (<100 Beds)",    "Ref",      "-",
                                           "Mid-Size (100-399 Beds)", "1.73**", "0.154",
                                                 "Large (>399 Beds)", "2.38**", "0.188",
                                                             "Rural",    "Ref",      "-",
                                                             "Urban", "1.53**", "0.161",
                                                              "HRRP",  "0.78*", "0.112",
                                                               "VBP", "1.61**", "0.085"
  )

require(knitr)
kable(Fine_All, digits = 3, row.names = FALSE, align = "c",
              caption = NULL)

```








